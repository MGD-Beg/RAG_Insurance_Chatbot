 # ğŸ§  Local RAG Chatbot (Gemma3:1B + Ollama + Streamlit)
 
 This project implements a **Retrieval-Augmented Generation (RAG) chatbot** using the **Gemma 3B model from Ollama**, powered by a vector store built from custom PDF documents. The chatbot can answer user questions using only the contents of the provided documents and responds with **"I don't know"** if the information is not found.
 
 ---
 
 ## ğŸ“Œ Features
 
 - ğŸ” Retrieves relevant content from PDF documentation.
 - ğŸ§  Uses local LLM (Gemma3:1B via [Ollama](https://ollama.com)) for generation.
 - âš¡ Fast semantic search with FAISS + Sentence-Transformers.
 - ğŸ–¥ï¸ User-friendly web interface with Streamlit.
 - ğŸš« No OpenAI or paid APIs â€“ fully local and free.
 
 ---
 
 ## ğŸ—‚ï¸ Project Structure
 
 ```
 RAG_Chatbot/
 â”‚
 â”œâ”€â”€ app.py                   # Streamlit app for chat interface
 â”œâ”€â”€ build_vector_store.py    # Script to parse PDFs and build FAISS index
 â”œâ”€â”€ vectordb.index           # FAISS vector index (generated)
 â”œâ”€â”€ chunks.pkl               # Pickled list of text chunks (generated)
 â”œâ”€â”€ requirements.txt         # Python dependencies
 â”œâ”€â”€ README.md                # You're here!
 â”‚
 â”œâ”€â”€ data/                    # Folder for all PDF source files
 â”‚   â”œâ”€â”€ some_document.pdf
 â”‚   â””â”€â”€ another_doc.pdf
 ```
 
 ---
 
 ## âœ… Requirements
 
 - Python 3.8+
 - [Ollama](https://ollama.com) (installed and running locally)
 - PDF documents placed in the `/data` folder
 
 ---
 
 ## ğŸ› ï¸ Installation
 
 ### 1. Clone the repository
 
 ```bash
 git clone https://github.com/yourname/local-rag-chatbot.git
 cd local-rag-chatbot
 ```
 
 ### 2. Create a virtual environment
 
 ```bash
 python -m venv venv
 source venv/bin/activate   # On Windows: venv\Scripts\activate
 ```
 
 ### 3. Install dependencies
 
 ```bash
 pip install -r requirements.txt
 ```
 
 If you donâ€™t have `requirements.txt`, install manually:
 
 ```bash
 pip install streamlit pymupdf faiss-cpu sentence-transformers ollama
 ```
 
 ---
 
 ## ğŸ¤– Setup Ollama with Gemma
 
 1. **Install Ollama** from [https://ollama.com](https://ollama.com)
 2. **Pull the Gemma model**:
 
 ```bash
 ollama pull gemma:2b
 ```
 
 3. **Start the model server**:
 
 ```bash
 ollama run gemma:2b
 ```
 
 Ollama will serve the model locally at `http://localhost:11434`.
 
 ---
 
 ## ğŸ“š Prepare the Knowledge Base
 
 1. Place your PDF documents inside the `data/` directory.
 2. Run the vector store builder:
 
 ```bash
 python build_vector_store.py
 ```
 
 This will:
 - Extract text from all PDFs in `/data`
 - Chunk the text into ~1000 characters
 - Create sentence embeddings
 - Save the FAISS index and chunk data as:
   - `vectordb.index`
   - `chunks.pkl`
 
 ---
 
 ## ğŸš€ Run the Chatbot App
 
 Start the Streamlit interface:
 
 ```bash
 streamlit run app.py
 ```
 
 Then open [http://localhost:8501](http://localhost:8501) in your browser.
 
 ---
 
 ## ğŸ’¬ How It Works
 
 1. You enter a question in the Streamlit app.
 2. Your question is embedded using `all-MiniLM-L6-v2`.
 3. FAISS retrieves the top 3 most relevant chunks from the PDFs.
 4. The context chunks and your question are sent to **Gemma** via **Ollama**.
 5. If the answer exists in the context, the model will respond.
 6. If the context doesn't contain an answer, it will respond: **"I don't know."**
 
 ---
 
 ## ğŸ§ª Example Questions
 
 - âœ… â€œWhat is the deductible for the Americaâ€™s Choice 2500 Gold plan?â€
 - âŒ â€œWho is the president of the US?â€ â†’ â€œI donâ€™t knowâ€
 
 ---
 
 ## ğŸ›‘ Windows Users â€“ Fix for Runtime Errors
 
 If you get this error:
 
 ```
 RuntimeError: no running event loop
 ```
 
 Add this to the top of `app.py`:
 
 ```python
 import sys, asyncio
 if sys.platform.startswith('win'):
     asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
 ```
 
 Also move this import **inside the button click handler** to prevent PyTorch/Streamlit clashes:
 
 ```python
 from sentence_transformers import SentenceTransformer
 ```
 
 ---
 
 ## ğŸ“„ License
 
 MGD License â€“ free to use and modify locally.
 
 ---
 
 ## ğŸ™‹â€â™‚ï¸ Credits
 
 - ğŸ’¬ LLM: [Gemma by Google](https://ollama.com/library/gemma)
 - ğŸ§  Host: [Ollama](https://ollama.com)
 - ğŸ§® Embeddings: [Sentence-Transformers](https://www.sbert.net/)
 - ğŸ§± Vector Search: [FAISS](https://github.com/facebookresearch/faiss)
 - ğŸ¨ UI: [Streamlit](https://streamlit.io)
 
 ---
